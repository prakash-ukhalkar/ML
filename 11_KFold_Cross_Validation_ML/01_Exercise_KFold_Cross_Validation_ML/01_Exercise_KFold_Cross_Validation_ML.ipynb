{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e8f813",
   "metadata": {},
   "source": [
    "### **K-Fold Cross-Validation for Diabetes Prediction**\n",
    "\n",
    "### Problem Statement: \n",
    "\n",
    "* The objective of this notebook is to build and evaluate machine learning models for predicting diabetes based on diagnostic measurements from the Pima Indians Diabetes Dataset. \n",
    "\n",
    "* We will use *K-Fold Cross-Validation* to ensure our model evaluation is robust and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620e72f",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Exploration\n",
    "\n",
    "* We will load the Pima Indians Diabetes dataset, which contains diagnostic information for 768 female patients. \n",
    "\n",
    "* The target variable is 'Outcome', where `1` indicates diabetes and `0` indicates no diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c68b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da63a62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>diabetes_pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnancies  glucose  blood_pressure  skin_thickness  insulin   bmi  \\\n",
       "0            6      148              72              35        0  33.6   \n",
       "1            1       85              66              29        0  26.6   \n",
       "2            8      183              64               0        0  23.3   \n",
       "3            1       89              66              23       94  28.1   \n",
       "4            0      137              40              35      168  43.1   \n",
       "\n",
       "   diabetes_pedigree  age  outcome  \n",
       "0              0.627   50        1  \n",
       "1              0.351   31        0  \n",
       "2              0.672   32        1  \n",
       "3              0.167   21        0  \n",
       "4              2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset directly from UCI repository\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "column_names = ['pregnancies', 'glucose', 'blood_pressure', 'skin_thickness', 'insulin', 'bmi', 'diabetes_pedigree', 'age', 'outcome']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe70beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df.drop('outcome', axis=1)\n",
    "y = df['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e51e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "Number of samples: 768\n",
      "Number of features: 8\n",
      "\n",
      "Target classes distribution:\n",
      "outcome\n",
      "0    500\n",
      "1    268\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(\"\\nTarget classes distribution:\")\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894d079",
   "metadata": {},
   "source": [
    "### Step 2: K-Fold Cross-Validation\n",
    "\n",
    "* K-Fold works by partitioning the entire dataset into K equal-sized folds. \n",
    "\n",
    "* In each iteration, one fold is set aside as the test set, while the remaining K-1 folds are combined to form the training set. \n",
    "\n",
    "* This process is repeated K times, with the final performance being the average score across all iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3f2b4",
   "metadata": {},
   "source": [
    "**Manual K-Fold Implementation**\n",
    "\n",
    "* We will manually implement `K-Fold` to see how the data is split and how the scores are calculated across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7e2311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.7532\n",
      "Fold 2 Accuracy: 0.7857\n",
      "Fold 3 Accuracy: 0.7532\n",
      "Fold 4 Accuracy: 0.8105\n",
      "Fold 5 Accuracy: 0.7451\n",
      "\n",
      "Average K-Fold Accuracy: 0.7696\n"
     ]
    }
   ],
   "source": [
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the model, Logistic Regression in this case\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create a list to store the scores\n",
    "scores = []\n",
    "\n",
    "# Perform K-Fold cross-validation and store the scores\n",
    "# Finally, print the results and the average accuracy\n",
    "for fold_num, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', model)])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {fold_num+1} Accuracy: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage K-Fold Accuracy: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9d2c2",
   "metadata": {},
   "source": [
    "### Step 3: Stratified K-Fold vs. K-Fold\n",
    "\n",
    "* The Pima Indians Diabetes dataset has a slight class imbalance, making **Stratified K-Fold** the superior technique. \n",
    "\n",
    "* Stratified K-Fold ensures that each fold maintains the same proportion of class labels as the original dataset. \n",
    "\n",
    "* This prevents a single fold from containing a disproportionately low or high number of samples from a particular class, which would lead to a biased performance evaluation. \n",
    "\n",
    "* Stratified K-Fold therefore provides a more reliable and stable score for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c6c21_md",
   "metadata": {},
   "source": [
    "**Manual Stratified K-Fold Implementation**\n",
    "\n",
    "* We'll now apply Stratified K-Fold to our problem to see its effect on the evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9c6c21_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy (Stratified): 0.7857\n",
      "Fold 2 Accuracy (Stratified): 0.7792\n",
      "Fold 3 Accuracy (Stratified): 0.7662\n",
      "Fold 4 Accuracy (Stratified): 0.7451\n",
      "Fold 5 Accuracy (Stratified): 0.7582\n",
      "\n",
      "Average Stratified K-Fold Accuracy: 0.7669\n"
     ]
    }
   ],
   "source": [
    "# Define the StratifiedKFold cross-validator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the model, Random Forest in this case\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create a list to store the scores of stratified K-Fold\n",
    "scores_stratified = []\n",
    "\n",
    "# Perform Stratified K-Fold cross-validation and store the scores\n",
    "# Finally, print the results and the average accuracy\n",
    "for fold_num, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', model)])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    scores_stratified.append(score)\n",
    "    print(f\"Fold {fold_num+1} Accuracy (Stratified): {score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Stratified K-Fold Accuracy: {np.mean(scores_stratified):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d2b78",
   "metadata": {},
   "source": [
    "### Step 4: Using `cross_val_score` for Simplicity\n",
    "\n",
    "* The `cross_val_score` function is a powerful utility from scikit-learn that automates the entire cross-validation process. \n",
    "\n",
    "* It handles the data splitting, model training, and scoring for each fold, making the process of model evaluation and comparison much more efficient. \n",
    "\n",
    "* For classification, it uses Stratified K-Fold by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86d267_md",
   "metadata": {},
   "source": [
    "**Comparing Multiple Models with `cross_val_score`**\n",
    "\n",
    "* Let's use `cross_val_score` to compare the performance of several different models on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d86d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Average Accuracy = 0.7709, Std Dev = 0.0247\n",
      "Random Forest: Average Accuracy = 0.7631, Std Dev = 0.0341\n",
      "Decision Tree: Average Accuracy = 0.7189, Std Dev = 0.0558\n",
      "SVM: Average Accuracy = 0.7722, Std Dev = 0.0248\n"
     ]
    }
   ],
   "source": [
    "# Define the multiple models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='liblinear'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "# Create a list to store the results\n",
    "results = {}\n",
    "\n",
    "# Perform Stratified K-Fold cross-validation using cross_val_score and store the scores\n",
    "# Finally, print the results and the average accuracy\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', model)])\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "    results[name] = scores\n",
    "    print(f\"{name}: Average Accuracy = {scores.mean():.4f}, Std Dev = {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d925d7b",
   "metadata": {},
   "source": [
    "### Step 5: Parameter Tuning with `cross_val_score`\n",
    "\n",
    "* Another key application of cross-validation is hyperparameter tuning. \n",
    "\n",
    "* We can evaluate a model's performance with different parameter settings to find the combination that provides the best generalized performance. \n",
    "\n",
    "* Here, we tune the `n_estimators` parameter for our `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8c0a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_estimators=20: Average Accuracy = 0.7630\n",
      "N_estimators=50: Average Accuracy = 0.7722\n",
      "N_estimators=100: Average Accuracy = 0.7631\n",
      "N_estimators=200: Average Accuracy = 0.7657\n",
      "\n",
      "Best N_estimators: 50 with an average score of 0.7722\n"
     ]
    }
   ],
   "source": [
    "# Define the range of n_estimators to test\n",
    "n_estimators_list = [20, 50, 100, 200]\n",
    "\n",
    "# Define variables to store the best n_estimators and score\n",
    "best_n_estimators = 0\n",
    "best_score = 0\n",
    "\n",
    "# Perform Stratified K-Fold cross-validation and store the scores\n",
    "# Finally, print the results and the average accuracy\n",
    "for n in n_estimators_list:\n",
    "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', model)])\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "    avg_score = scores.mean()\n",
    "    print(f\"N_estimators={n}: Average Accuracy = {avg_score:.4f}\")\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_n_estimators = n\n",
    "\n",
    "print(f\"\\nBest N_estimators: {best_n_estimators} with an average score of {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d3e7b",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* Based on the cross-validation results from this notebook, we can draw the following conclusions:\n",
    "\n",
    "  1. **Importance of Cross-Validation:** Using a cross-validation approach provided a stable and reliable performance metric, which is crucial for a dataset like the Pima Indians Diabetes Dataset. It prevents the model evaluation from being skewed by a single train-test split.\n",
    "\n",
    "  2. **Model Performance:** Among the models evaluated, the Random Forest Classifier demonstrated the highest average accuracy. This is a common finding, as ensemble methods like Random Forest are highly effective.\n",
    "\n",
    "  3. **Hyperparameter Tuning:** Our analysis using `cross_val_score` showed a clear trend: increasing the number of trees (`n_estimators`) in the Random Forest model improved its performance up to a certain point. This confirms that proper hyperparameter tuning is a critical step for maximizing a model's predictive power.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074a54f",
   "metadata": {},
   "source": [
    "*Machine Learning - Python Notebook* by [*Prakash Ukhalkar*](https://github.com/prakash-ukhalkar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
